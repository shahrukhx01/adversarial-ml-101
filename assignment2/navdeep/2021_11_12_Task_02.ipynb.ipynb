{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NzyqehHjUlG"
   },
   "source": [
    "# ML in Cybersecurity: Task II\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  *R2D2C3P0BB8*\n",
    "  * **Members**:  <br/> **Navdeeppal Singh (s8nlsing@stud.uni-saarland.de)** <br/> **Shahrukh Khan (shkh00001@stud.uni-saarland.de)** <br/> **Mahnoor Shahid (mash00001@stud.uni-saarland.de)**\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 25th Nov. 2021, 23:59:59 (email the completed notebook including outputs to mlcysec_ws2022_staff@lists.cispa.saarland)\n",
    "  * Email the completed notebook to mlcysec_ws2022_staff@lists.cispa.saarland \n",
    "  * Complete this in the previously established **teams of 3**\n",
    "  * Feel free to use the course forum to discuss.\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, we dive into the vulnerabilities of machine learning models and the difficulties of defending against them. To this end, we ask you to implement an evasion attack (craft adversarial examples) yourselves, and defend your own model.   \n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The total number of points in this project is 100. We further provide the number of points achievable with each excercise. You should take particular care to document and visualize your results.\n",
    "\n",
    "Whenever possible, please use tools like tables or figures to compare the different findings\n",
    "\n",
    "\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with (all) your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ewNwfFvbFaR"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "640GrzbOevr0"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, SubsetRandomSampler\n",
    "# Please set random seed to have reproduceable results, e.g. torch.manual_seed(123)\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJZPEAWYMhYB"
   },
   "outputs": [],
   "source": [
    "compute_mode = 'cpu'\n",
    "\n",
    "if compute_mode == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "elif compute_mode == 'gpu':\n",
    "    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n",
    "    # It is extremely important that you *do not* spawn multi-GPU jobs.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'    # Set device ID here\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('Unrecognized compute mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxi-lLD0mKHD"
   },
   "source": [
    "#### Helpers\n",
    "\n",
    "In case you choose to have some methods you plan to reuse during the notebook, define them here. This will avoid clutter and keep rest of the notebook succinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBbigqdEmKd8"
   },
   "outputs": [],
   "source": [
    "# data loading helper\n",
    "def _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    This method is created to split the MNIST data into training, validation and testing set accordingly \n",
    "    and load it into dataloaders. Also, to specify any transformations required to perform on the data. \n",
    "    As well as this method is being called multiple times in hyper parameter tuning where different batch \n",
    "    sizes are being tested\n",
    "    ...\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    DATA_PATH : str\n",
    "        specifies the path directory where dataset will be downloaded\n",
    "    TRAIN_BATCH_SIZE : int\n",
    "        specifies the batch size in the training loader\n",
    "    TEST_BATCH_SIZE : int\n",
    "        specifies the batch size in the training loader\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_loader, validation_loader, test_loader with the specified batch sizes \n",
    "        \n",
    "    \"\"\"\n",
    "    tranformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mnist_training_dataset = datasets.MNIST(\n",
    "        root=DATA_PATH + \"train\", train=True, download=True, transform=tranformations\n",
    "    )\n",
    "    mnist_testing_dataset = datasets.MNIST(\n",
    "        root=DATA_PATH + \"test\", train=False, download=True, transform=tranformations\n",
    "    )\n",
    "\n",
    "    training_dataset, validation_dataset = random_split(\n",
    "        dataset=mnist_training_dataset,\n",
    "        lengths=[\n",
    "            int(0.8 * len(mnist_training_dataset)),\n",
    "            int(0.2 * len(mnist_training_dataset)),\n",
    "        ],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "        # TODO: ASK: We should keep the *output* of the **function** deterministic, no?\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True\n",
    "    )\n",
    "    validation_loader = DataLoader(\n",
    "        training_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        mnist_testing_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "\n",
    "## data sample helper\n",
    "def _get_test_data_sample(DATA_PATH, TEST_BATCH_SIZE=1, SAMPLE_SIZE=1000):\n",
    "    \"\"\"\n",
    "    This method creates a subset from testset for creating adversarial examples\n",
    "    ...\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    DATA_PATH : str\n",
    "        specifies the path directory where dataset will be downloaded\n",
    "    TRAIN_BATCH_SIZE : int\n",
    "        specifies the batch size in the test subset loader\n",
    "    SAMPLE_SIZE : int\n",
    "        specifies the sample size of the subset\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    test_subset_loader\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    tranformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mnist_testing_dataset = datasets.MNIST(\n",
    "        root=osp.join(DATA_PATH, \"test\"),\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=tranformations,\n",
    "    )\n",
    "\n",
    "    N = len(mnist_testing_dataset)\n",
    "\n",
    "    assert N >= SAMPLE_SIZE, f\"{SAMPLE_SIZE=} can't be bigger than Dataset size ({N=})!\"\n",
    "\n",
    "    sampled_subset = random_split(\n",
    "        dataset=mnist_testing_dataset,\n",
    "        lengths=[SAMPLE_SIZE, N - SAMPLE_SIZE],\n",
    "        generator=torch.Generator().manual_seed(1337),\n",
    "        # TODO: same here as above â¤´\n",
    "    )\n",
    "\n",
    "    test_subset_loader = DataLoader(\n",
    "        sampled_subset, batch_size=TEST_BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    return test_subset_loader\n",
    "\n",
    "\n",
    "def carry_attack(\n",
    "    attack: \"Callable\",\n",
    "    model: nn.Module,\n",
    "    epsilon: float,\n",
    "    dataloader: DataLoader,\n",
    "    progress: bool = False,\n",
    "    **kwargs,\n",
    ") -> tuple[list[torch.Tensor], list[int], list[int], list[int]]:\n",
    "    \"\"\"\n",
    "    Carries given adversarial attack on model for given perturbation budget ($L_\\infty$). \n",
    "\n",
    "    Args:\n",
    "        attack (Callable): The attack to use.\n",
    "        model (nn.Module): The model to attack.\n",
    "        epsilons (float): The perturbation budget.\n",
    "        dataloader (DataLoader): The data loader to use.\n",
    "        progress (bool, optional): Whether or not to show progress. Default to False.\n",
    "        **kwargs: Key-word arguments to give to the attack. E.g. the number of steps, the step-size, etc.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[torch.Tensor], list[int], list[int], list[int]]: Tuple of results. \n",
    "            adversarial examples, adversarial predictions, clean predictions, and ground truth label.\n",
    "    \"\"\"\n",
    "    assert callable(attack), f\"Given attack should be callable! Got {attack}\"\n",
    "    assert epsilon > 0, \"Give at least 1 epsilon!\"\n",
    "    assert not model.training, \"Model is in training mode!\"\n",
    "\n",
    "    # ---\n",
    "    adv_images = []\n",
    "    adv_predictions = []\n",
    "    clean_predictions = []\n",
    "    ground_labels = []  # for comparison's sake\n",
    "\n",
    "    # go through dataset\n",
    "    for x, y in tqdm(dataloader, disable=not progress):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            y_clean = model(x).argmax().item()\n",
    "\n",
    "        # attack and predict\n",
    "        x_adv = attack(model=model, epsilon=epsilon, x=x, y=y, **kwargs)\n",
    "        y_adv_pred = model(x_adv).argmax().item()\n",
    "\n",
    "        # store\n",
    "        adv_images.append(x_adv.cpu().numpy())\n",
    "        adv_predictions.append(y_adv_pred)\n",
    "        clean_predictions.append(y_clean)\n",
    "        ground_labels.append(y.item())\n",
    "\n",
    "    return adv_images, adv_predictions, clean_predictions, ground_labels\n",
    "\n",
    "\n",
    "def evaluate_adversarial_results(\n",
    "    clean_predictions: list[int], adv_predictions: list[int], ground_labels: list[int],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For printing the accuracy metrics.\n",
    "\n",
    "    Args:\n",
    "        clean_predictions (list[int]): Predictions on the clean examples.\n",
    "        adv_predictions (list[int]): Predictions on the adversarial examples.\n",
    "        ground_labels (list[int]): The true labels.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(clean_predictions) == len(adv_predictions) == len(ground_labels),\n",
    "        f\"All arguments should have equal lengths! Got [{len(clean_predictions)}, {len(adv_predictions)}, {len(ground_labels)}]\",\n",
    "    )\n",
    "\n",
    "    N = len(clean_predictions)\n",
    "\n",
    "    ground_labels = np.array(ground_labels)\n",
    "    correct_clean = sum(np.array(clean_predictions) == ground_labels)\n",
    "    correct_adv = sum(np.array(adv_predictions) == ground_labels)\n",
    "\n",
    "    print(f\"Clean accuracy on attack set is: {correct_clean / N:.2f}\")\n",
    "    print(f\"Robust accuracy on attack set is: {correct_adv / N:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1pcmKkyjT7y"
   },
   "source": [
    "# 1. Attacking an ML-model (30 points) \n",
    "\n",
    "In this section, we implement an attack ourselves. First, however, you need a model you can attack. Feel free to choose the DNN/ConvNN from task 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QaJv_d_Dp7OM"
   },
   "source": [
    "## 1.1: Setting up the model and data (4 Points)\n",
    "\n",
    "Load the MNIST data, as done in task 1. \n",
    "\n",
    "Re-use the model from task 1 here and train it until it achieves reasonable accuracy (>92%).\n",
    "\n",
    "If you have the saved checkpoint from task 1, you can load it directly. But please compute here the test accuracy using this checkpoint.  \n",
    "\n",
    "**Hint:** In order to save computation time for the rest of exercise, you might consider having a relatively small model here.\n",
    "\n",
    "**Hint**: You might want to save the trained model to save time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Loading data\n",
    "\n",
    "DATA_PATH = './data/'\n",
    "TRAIN_BATCH_SIZE, TEST_BATCH_SIZE = 64, 64\n",
    "_, _, test_loader = _get_data(DATA_PATH, TRAIN_BATCH_SIZE, TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Defining model\n",
    "\n",
    "\n",
    "class CNN_Network(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        \"\"\"\n",
    "        This class is created to specify the Convolutional Neural Network on which MNIST dataset is trained on, \n",
    "        validated and later tested. \n",
    "        It consist of one input layer, one output layer can consist of multiple hidden layers all of which is \n",
    "        specified by the user as provided through model_paramaters\n",
    "        Size of the kernel, stride and padding can also be adjusted by the user as provided through model_paramaters\n",
    "        ...\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_params : dictionary\n",
    "            provides the model with the required input size, hidden layers and output size\n",
    "            \n",
    "            model_params = {\n",
    "            'INPUT_SIZE' : int,\n",
    "            'HIDDEN_LAYERS' : list(int),\n",
    "            'OUTPUT_SIZE' : int,\n",
    "            'KERNEL' : int,\n",
    "            'STRIDE' : int,\n",
    "            'PADDING' : int\n",
    "        }\n",
    "        \"\"\"\n",
    "        super(CNN_Network, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        for input_channel, out_channel in zip(\n",
    "            [model_params[\"INPUT_SIZE\"]] + model_params[\"HIDDEN_LAYERS\"][:-1],\n",
    "            model_params[\"HIDDEN_LAYERS\"][: len(model_params[\"HIDDEN_LAYERS\"])],\n",
    "        ):\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    input_channel,\n",
    "                    out_channel,\n",
    "                    model_params[\"KERNEL\"],\n",
    "                    model_params[\"STRIDE\"],\n",
    "                    model_params[\"PADDING\"],\n",
    "                    bias=True,\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.MaxPool2d(2, 2))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        \n",
    "        layers.append(nn.Flatten(1))\n",
    "        layers.append(\n",
    "            nn.Linear(\n",
    "                model_params[\"HIDDEN_LAYERS\"][-1],\n",
    "                model_params[\"OUTPUT_SIZE\"],\n",
    "                bias=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. initializing the pre-trained model from assignment 1\n",
    "model_params = {\n",
    "    \"INPUT_SIZE\": 1,\n",
    "    \"HIDDEN_LAYERS\": [160, 100, 64, 10],\n",
    "    \"OUTPUT_SIZE\": 10,\n",
    "    \"KERNEL\": 3,\n",
    "    \"STRIDE\": 1,\n",
    "    \"PADDING\": 1,\n",
    "}\n",
    "\n",
    "undefended_model = CNN_Network(model_params).to(compute_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading checkpoint and evaluating on test set\n",
    "def _test_model(model, test_loader, BEST_MODEL):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(BEST_MODEL, map_location=compute_mode))\n",
    "        with torch.no_grad():\n",
    "            correct_predictions = []\n",
    "            testing_acc_scores = []\n",
    "            wrong_predictions = []\n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "\n",
    "            for images, targets in iter(test_loader):\n",
    "                images = images.to(compute_mode)\n",
    "                targets = targets.to(compute_mode)\n",
    "                outputs = model(images)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_indicies = (preds == targets).nonzero(as_tuple=True)[0]\n",
    "                c_images = images[correct_indicies]\n",
    "                c_targets = targets[correct_indicies]\n",
    "                c_correct_preds = preds[correct_indicies]\n",
    "                testing_acc_scores.append(len(correct_indicies) / targets.shape[0])\n",
    "\n",
    "                wrong_indicies = (preds != targets).nonzero(as_tuple=True)[0]\n",
    "                w_images = images[wrong_indicies]\n",
    "                w_targets = targets[wrong_indicies]\n",
    "                w_wrong_preds = preds[wrong_indicies]\n",
    "\n",
    "                correct_predictions += zip(c_images, c_targets, c_correct_preds)\n",
    "                wrong_predictions += zip(w_images, w_targets, w_wrong_preds)\n",
    "                all_targets += zip(targets.cpu().numpy())\n",
    "                all_preds += zip(preds.cpu().numpy())\n",
    "\n",
    "            return (\n",
    "                (sum(testing_acc_scores) / len(testing_acc_scores)) * 100,\n",
    "                correct_predictions,\n",
    "                wrong_predictions,\n",
    "                all_targets,\n",
    "                all_preds,\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occured in testing the model = \", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    test_accuracy,\n",
    "    correct_predictions,\n",
    "    wrong_predictions,\n",
    "    all_targets,\n",
    "    all_preds,\n",
    ") = _test_model(\n",
    "    undefended_model,\n",
    "    test_loader,\n",
    "    BEST_MODEL=\"Accuracy_99.03125_batchsize_64_lr_0.001.ckpt\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEQrdyLHsUIu"
   },
   "source": [
    "## 1.2: Implementing the FGSM attack (7 Points)\n",
    "\n",
    "We now want to attack the model trained in the previous step. We will start with the FGSM attack as a simple example. \n",
    "\n",
    "Please implement the FGSM attack mentioned in the lecture. \n",
    "\n",
    "More details: https://arxiv.org/pdf/1412.6572.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcVZnUNbRKOz"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNpI3oUoO1wE"
   },
   "source": [
    "## 1.3: Adversarial sample set (7 Points)\n",
    "\n",
    "* Please generate a dataset containing at least 1,000 adversarial examples using FGSM.\n",
    "\n",
    "* Please vary the perturbation budget (3 variants) and generate 1,000 adversarial examples for each. \n",
    "    * **Hint**: you can choose epsilons within, e.g., = [.05, .1, .15, .2, .25, .3],  using MNIST pixel values in the interval       [0, 1]\n",
    "\n",
    "* Compute the accuracy of each attack set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvYpo9p2O1wF"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ex3qQp3JolD1"
   },
   "source": [
    "## 1.4: Visualizing the results (7 Points)\n",
    "\n",
    "* Please chose one sample for each class (for example the first when iterating the test data) and plot the (ten) adversarial examples as well as the predicted label (before and after the attack)\n",
    "\n",
    "* Please repeat the visualization for the three sets you have created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGkp0B0PO1wJ"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# template code (Please feel free to change this)\n",
    "# (each column corresponds to one attack method)\n",
    "col_titles = [\"Ori\", \"FGSM\", \"Method 1\", \"Method 2\"]\n",
    "nsamples = 10\n",
    "nrows = nsamples\n",
    "ncols = len(col_titles)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows, ncols, figsize=(8, 12)\n",
    ")  # create the figure with subplots\n",
    "[ax.set_axis_off() for ax in axes.ravel()]  # remove the axis\n",
    "\n",
    "for ax, col in zip(axes[0], col_titles):  # set up the title for each column\n",
    "    ax.set_title(col, fontdict={\"fontsize\": 18, \"color\": \"b\"})\n",
    "\n",
    "for i in range(nsamples):\n",
    "    axes[i, 0].imshow(images_ori[i])\n",
    "    axes[i, 1].imshow(adv_FGSM[i])\n",
    "    axes[i, 2].imshow(adv_Method1[i])\n",
    "    axes[i, 3].imshow(adv_Method2[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5: Analyzing the results (5 Points)\n",
    "\n",
    "Please write a brief summary of your findings.  \n",
    "\n",
    "* Does the attack always succeed (the model makes wrong prediction on the adversarial sample)? What is the relationship between the attack success rate and the perturbation budget?\n",
    "* How about the computation cost of the attack? (you can report the time in second) \n",
    "* Does the attack require white-box access to the model?\n",
    "* Feel free to report your results via tables or figures, and mention any other interesting observations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJUmrv5Bymij"
   },
   "source": [
    "# 2. Defending an ML model (35 points) \n",
    "\n",
    "So far, we have focused on attacking an ML model. In this section, we want you to defend your model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gHUFK6Mymik"
   },
   "source": [
    "## 2.1: Implementing the adversarial training defense (20 Points)\n",
    "\n",
    "* We would like to ask you to implement the adversarial training defense (https://arxiv.org/pdf/1412.6572.pdf) mentioned in the lecture. \n",
    "\n",
    "* You can use the **FGSM adversarial training** method (i.e., train on FGSM examples). \n",
    "\n",
    "* You can also check the adversarial training implementation in other papers, e.g., http://proceedings.mlr.press/v97/pang19a/pang19a.pdf \n",
    "\n",
    "* Choose a certain **maximum perturbation budget** during training that is in the middle of the range you have experimented with before. \n",
    "\n",
    "* We do not require the defense to work perfectly - but what we want you to understand is why it works or why it does not work.\n",
    "\n",
    "**Hint:** You can save the checkpoint of the defended model as we would need it to for the third part of this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DD0UalSeymim"
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Evaluation (10 Points)\n",
    "\n",
    "* Craft adversarial examples using the **defended** model. This entails at least 1,000 examples crafted via FGSM. \n",
    "    * Create one set using a budget that is **less than (within)** the one used in training.\n",
    "    * Create another set using a budget that is **higher than** the one used in training. \n",
    "    * You can use two values of epsilons from question 1.3 \n",
    "    \n",
    "* Evaluate the **defended** model on these two adversarial examples sets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "print(\"Accuracy on the lower-budget adversarial samples (FGSM) %.2f\" % acc_FGSM1)\n",
    "print(\n",
    "    \"Accuracy on the lower-budget adversarial samples (FGSM) after defense %.2f\"\n",
    "    % acc_FGSM_defend1\n",
    ")\n",
    "\n",
    "print(\"Accuracy on the higher-budget adversarial samples (FGSM) %.2f\" % acc_FGSM2)\n",
    "print(\n",
    "    \"Accuracy on the higher-budget adversarial samples (FGSM) after defense %.2f\"\n",
    "    % acc_FGSM_defend2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Discussion (5 points)\n",
    "\n",
    "* How successful was the defense against the attack compared to the undefended model? How do you interpret the difference?\n",
    "* How did the two sets differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: I-FGSM attack (35 points) \n",
    "\n",
    "* FGSM is one of the simplest and earliest attacks. Since then, many more advanced attacks have been proposed. \n",
    "* One of them is the [Iterative-FGSM](https://arxiv.org/pdf/1607.02533.pdf), where the attack is repeated multiple times.\n",
    "* In this part, we ask you to please implement the iterative FGSM attack. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Implementing the I-FGSM attack (10 Points)\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "* Your code should have an attack loop. At each step, the FGSM attack that you have implemented before is computed using a small step.\n",
    "* After each step, you should perform a per-pixel clipping to make sure the image is in the allowed range, and that the perturbation is within budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "def attack_ifgsm(\n",
    "    model: nn.Module,\n",
    "    epsilon: float,\n",
    "    steps: int,\n",
    "    step_size: float,\n",
    "    x: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    progress: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Iterative-FGSM attack.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to attack.\n",
    "        epsilon (float): The epsilon bound to use.\n",
    "        steps (int): The number of steps/iterations to make.\n",
    "        x (torch.Tensor): The images to create adversarial examples out of.\n",
    "        y (torch.Tensor): The labels of said images.\n",
    "        progress (bool, optional): Whether to show a progress bar or not. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: The adversarial examples generated.\n",
    "    \"\"\"\n",
    "    assert not model.training, \"Model is in training mode!\"\n",
    "    assert epsilon > 0, \"Epsilon value must be greater than 0!\"\n",
    "    assert steps > 0, \"Requires a positive number of steps!\"\n",
    "    assert step_size > 0, \"Step-size should be positive!\"\n",
    "    assert x.max() <= 1, \"Make sure image values are in range [0,1]!\"\n",
    "    assert x.min() >= 0, \"Make sure image values are in range [0,1]!\"\n",
    "\n",
    "    # ---\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    step_size = epsilon / steps\n",
    "\n",
    "    # running variable for the adversarial example(s)\n",
    "    x_adv = x.detach().clone()\n",
    "    x_adv.requires_grad = True\n",
    "\n",
    "    for i in tqdm(range(steps), disable=not progress):\n",
    "\n",
    "        # zero out gradients b/w iterations\n",
    "        if i > 0:\n",
    "            x_adv.grad.zero_()\n",
    "\n",
    "        # get gradients\n",
    "        y_pred = model(x_adv)\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # calculate and update perturbation\n",
    "        delta = (x_adv - x) + (step_size * x_adv.grad.sign())\n",
    "\n",
    "        # clip to L_\\infty neighbourhood\n",
    "        delta = delta.clamp(min=-epsilon, max=epsilon)\n",
    "\n",
    "        # make sure result is still in [0, 1]\n",
    "        x_adv.data = (x + delta).clamp(0, 1)\n",
    "\n",
    "    return x_adv.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Attack the undefended model (5 Points)\n",
    "\n",
    "* We will first attack the **undefended model** (i.e., without adversarial training).\n",
    "\n",
    "* Choose one perturbation budget from Question **1.3** for comparison. \n",
    "\n",
    "    * Hint: A simple way to choose the small step is to divide the total budget by the number of steps (e.g., 10).\n",
    "\n",
    "* Please generate 1000 adversarial examples using the **undefended** model and the **I-FGSM** you implemented. \n",
    "\n",
    "* Please compute the accuracy of the adversarial set on the **undefended** model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "\n",
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1: Findings and comparison with FGSM (8 points)\n",
    "\n",
    "* Please report your findings. How successful was the attack? \n",
    "\n",
    "* What do you expect when increasing the number of steps? (you can experiment with different parameters of the attack and report your findings) \n",
    "\n",
    "* Compare with the basic FGSM. Using the same perturbation budget and using the same model, which attack is more successful? Why do you think this is the case? What about the computation time?\n",
    "\n",
    "* Feel free to report any interesting observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Attack the defended model (5 poinst) \n",
    "\n",
    "* In the previous question, we attacked the **undefended model**. \n",
    "\n",
    "* Now, we want to explore how successful the previous implemented defense (FGSM adversarial training) is againts this new attack. (we will not implement a new defense here, we will be reusing your previous checkpoint of the **defended model**)\n",
    "\n",
    "\n",
    "* Use the **defended model** to create one set of adversarial examples. Use a perturbation budget from Question **2.2** for comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1: Discussion (7 points) \n",
    "* Please report your results. How successful was the attack on the defended model? \n",
    "* Compare it with the success of the FGSM attack on the defended model. What do you observe? How do you interpret the difference? \n",
    "* How do you think you can improve the defense against I-FGSM attack?\n",
    "\n",
    "\n",
    "* Feel free to state any interesting findings you encountered during this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers go here**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_3_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
